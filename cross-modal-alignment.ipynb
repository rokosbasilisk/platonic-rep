{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efcd957-e48c-45ba-bab2-e5e470454b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 23:30:14,653 [INFO] Loading Conceptual Captions dataset for alignment...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column image not in the dataset. Current columns in the dataset: ['image_url', 'caption', 'labels', 'MIDs', 'confidence_scores']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Conceptual Captions dataset for alignment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-research-datasets/conceptual_captions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabeled\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:1000]\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Using 1000 samples for faster processing\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m images \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     59\u001b[0m captions \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     60\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m image-caption pairs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:2742\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:2726\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2724\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2725\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2726\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m   2727\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2728\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2729\u001b[0m )\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/datasets/formatting/formatting.py:590\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    588\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 590\u001b[0m     _check_valid_column_key(key, table\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/datasets/formatting/formatting.py:527\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column image not in the dataset. Current columns in the dataset: ['image_url', 'caption', 'labels', 'MIDs', 'confidence_scores']\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoImageProcessor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from metrics import AlignmentMetrics  # Ensure this is correctly implemented/imported\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gc\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "# -------------------- Setup Logging --------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "# Configuration for the Vision Model (ViT)\n",
    "VIT_MODEL_NAME = \"timm/vit_base_patch16_224\"\n",
    "VIT_FEATURES_DIR = \"vit_features\"\n",
    "VIT_CHECKPOINT_DIR = \"./vit_checkpoints\"\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 100\n",
    "SAVE_EVERY = 5  # Save every 5 epochs to ensure at least 20 checkpoints\n",
    "\n",
    "# Configuration for the Language Model (LLM)\n",
    "LLM_FEATURES_DIR = \"llm_features\"\n",
    "LLM_MODEL_CHECKPOINTS = {\n",
    "    \"pythia-160m\": [\n",
    "        \"step0\", \"step1000\", \"step8000\", \"step15000\", \"step22000\", \"step29000\", \"step36000\",\n",
    "        \"step43000\", \"step50000\", \"step57000\", \"step64000\", \"step71000\", \"step78000\",\n",
    "        \"step85000\", \"step92000\", \"step99000\", \"step106000\", \"step113000\", \"step120000\",\n",
    "        \"step127000\", \"step134000\", \"step143000\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(VIT_FEATURES_DIR, exist_ok=True)\n",
    "os.makedirs(LLM_FEATURES_DIR, exist_ok=True)\n",
    "os.makedirs(VIT_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Load an image-captioning dataset (Conceptual Captions)\n",
    "logger.info(\"Loading Conceptual Captions dataset for alignment...\")\n",
    "dataset = load_dataset(\"google-research-datasets/conceptual_captions\", \"labeled\", split=\"train[:1000]\")  # Using 1000 samples for faster processing\n",
    "images = dataset[\"image\"]\n",
    "captions = dataset[\"caption\"]\n",
    "logger.info(f\"Loaded {len(images)} image-caption pairs.\")\n",
    "\n",
    "# -------------------- Feature Extraction Functions --------------------\n",
    "\n",
    "def extract_vit_features(model, processor, images, batch_size=64):\n",
    "    \"\"\"Extract last hidden state features from the ViT model for given images.\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=\"Extracting VIT features\", leave=False):\n",
    "            batch_images = images[i:i + batch_size]\n",
    "            inputs = processor(batch_images, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            cls_tokens = last_hidden_state[:, 0, :]  # Extract CLS token\n",
    "            features.append(cls_tokens.cpu())\n",
    "\n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "def extract_llm_features(model, tokenizer, captions, batch_size=64):\n",
    "    \"\"\"Extract last hidden state features from the LLM for given captions.\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(captions), batch_size), desc=\"Extracting LLM features\", leave=False):\n",
    "            batch_captions = captions[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            cls_tokens = last_hidden_state[:, 0, :]  # Extract CLS token\n",
    "            features.append(cls_tokens.cpu())\n",
    "\n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "# -------------------- Training the ViT Model on CIFAR-10 --------------------\n",
    "\n",
    "def train_vit_on_cifar():\n",
    "    \"\"\"Train ViT on CIFAR-10 and save checkpoints.\"\"\"\n",
    "    model = timm.create_model(VIT_MODEL_NAME, pretrained=True, num_classes=10)\n",
    "    processor = AutoImageProcessor.from_pretrained(VIT_MODEL_NAME)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=\"./cifar_data\", train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Save checkpoint every SAVE_EVERY epochs\n",
    "        if (epoch + 1) % SAVE_EVERY == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "            checkpoint_path = os.path.join(VIT_CHECKPOINT_DIR, f\"vit_epoch_{epoch + 1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            logger.info(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        logger.info(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -------------------- Compute Alignment Scores --------------------\n",
    "\n",
    "def compute_alignment_between_vit_llm(vit_features, llm_features):\n",
    "    \"\"\"Compute alignment metrics between ViT and LLM features.\"\"\"\n",
    "    alignment_score = AlignmentMetrics.mutual_knn(vit_features, llm_features, topk=50)\n",
    "    return alignment_score\n",
    "\n",
    "def process_alignment_scores(vit_checkpoints, llm_model_checkpoints, images, captions):\n",
    "    \"\"\"Extract features from each model and compute alignment scores.\"\"\"\n",
    "    alignment_records = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Extract ViT features from saved checkpoints\n",
    "    processor = AutoImageProcessor.from_pretrained(VIT_MODEL_NAME)\n",
    "    for vit_epoch in vit_checkpoints:\n",
    "        checkpoint_path = os.path.join(VIT_CHECKPOINT_DIR, f\"vit_epoch_{vit_epoch}.pth\")\n",
    "        model = timm.create_model(VIT_MODEL_NAME, pretrained=False, num_classes=10)\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "        vit_features = extract_vit_features(model, processor, images)\n",
    "        vit_save_path = os.path.join(VIT_FEATURES_DIR, f\"vit_epoch_{vit_epoch}.pt\")\n",
    "        torch.save(vit_features, vit_save_path)\n",
    "\n",
    "        for llm_name, checkpoints in llm_model_checkpoints.items():\n",
    "            for checkpoint in checkpoints:\n",
    "                # Load LLM and extract features\n",
    "                llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    f\"EleutherAI/{llm_name}\", revision=checkpoint\n",
    "                ).to(device)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/{llm_name}\")\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                llm_features = extract_llm_features(llm_model, tokenizer, captions)\n",
    "                \n",
    "                # Compute alignment\n",
    "                alignment_score = compute_alignment_between_vit_llm(vit_features, llm_features)\n",
    "                alignment_records.append({\n",
    "                    'VIT Epoch': vit_epoch,\n",
    "                    'LLM Name': llm_name,\n",
    "                    'LLM Checkpoint': checkpoint,\n",
    "                    'Alignment Score': alignment_score\n",
    "                })\n",
    "                logger.info(f\"Alignment score between ViT (epoch {vit_epoch}) and {llm_name} ({checkpoint}): {alignment_score}\")\n",
    "\n",
    "    return pd.DataFrame(alignment_records)\n",
    "\n",
    "# -------------------- Plotting the Results --------------------\n",
    "\n",
    "def plot_alignment_scores(alignment_df):\n",
    "    \"\"\"Plot alignment scores over training steps.\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.lineplot(\n",
    "        data=alignment_df,\n",
    "        x='LLM Checkpoint',\n",
    "        y='Alignment Score',\n",
    "        hue='VIT Epoch',\n",
    "        marker='o'\n",
    "    )\n",
    "    plt.title('Alignment Scores between ViT and LLM Checkpoints')\n",
    "    plt.xlabel('LLM Training Step')\n",
    "    plt.ylabel('Alignment Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='ViT Epoch')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------- Main Execution --------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_vit_on_cifar()\n",
    "    vit_checkpoints = list(range(5, NUM_EPOCHS + 1, SAVE_EVERY))  # Epoch checkpoints for ViT\n",
    "    alignment_df = process_alignment_scores(vit_checkpoints, LLM_MODEL_CHECKPOINTS, images, captions)\n",
    "    alignment_df.to_csv('alignment_scores_vit_llm.csv', index=False)\n",
    "    plot_alignment_scores(alignment_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da989e41-f704-4be4-b0a5-9ed78794b90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
