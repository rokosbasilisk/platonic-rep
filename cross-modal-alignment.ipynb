{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcd957-e48c-45ba-bab2-e5e470454b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 23:25:00,965 [INFO] Loading Conceptual Captions dataset for alignment...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c26e91cff84687a53f4f0bcad86898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dac8646f5e14a00bbd588167ddfba01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/178M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoImageProcessor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from metrics import AlignmentMetrics  # Ensure this is correctly implemented/imported\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gc\n",
    "import logging\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import timm\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "# -------------------- Setup Logging --------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "# Configuration for the Vision Model (ViT)\n",
    "VIT_MODEL_NAME = \"timm/vit_base_patch16_224\"\n",
    "VIT_FEATURES_DIR = \"vit_features\"\n",
    "VIT_CHECKPOINT_DIR = \"./vit_checkpoints\"\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 100\n",
    "SAVE_EVERY = 5  # Save every 5 epochs to ensure at least 20 checkpoints\n",
    "\n",
    "# Configuration for the Language Model (LLM)\n",
    "LLM_FEATURES_DIR = \"llm_features\"\n",
    "LLM_MODEL_CHECKPOINTS = {\n",
    "    \"pythia-160m\": [\n",
    "        \"step0\", \"step1000\", \"step8000\", \"step15000\", \"step22000\", \"step29000\", \"step36000\",\n",
    "        \"step43000\", \"step50000\", \"step57000\", \"step64000\", \"step71000\", \"step78000\",\n",
    "        \"step85000\", \"step92000\", \"step99000\", \"step106000\", \"step113000\", \"step120000\",\n",
    "        \"step127000\", \"step134000\", \"step143000\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(VIT_FEATURES_DIR, exist_ok=True)\n",
    "os.makedirs(LLM_FEATURES_DIR, exist_ok=True)\n",
    "os.makedirs(VIT_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Load an image-captioning dataset (Conceptual Captions)\n",
    "logger.info(\"Loading Conceptual Captions dataset for alignment...\")\n",
    "dataset = load_dataset(\"google-research-datasets/conceptual_captions\", \"labeled\", split=\"validation[:1000]\")  # Using 1000 samples for faster processing\n",
    "images = dataset[\"image\"]\n",
    "captions = dataset[\"caption\"]\n",
    "logger.info(f\"Loaded {len(images)} image-caption pairs.\")\n",
    "\n",
    "# -------------------- Feature Extraction Functions --------------------\n",
    "\n",
    "def extract_vit_features(model, processor, images, batch_size=64):\n",
    "    \"\"\"Extract last hidden state features from the ViT model for given images.\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=\"Extracting VIT features\", leave=False):\n",
    "            batch_images = images[i:i + batch_size]\n",
    "            inputs = processor(batch_images, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            cls_tokens = last_hidden_state[:, 0, :]  # Extract CLS token\n",
    "            features.append(cls_tokens.cpu())\n",
    "\n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "def extract_llm_features(model, tokenizer, captions, batch_size=64):\n",
    "    \"\"\"Extract last hidden state features from the LLM for given captions.\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(captions), batch_size), desc=\"Extracting LLM features\", leave=False):\n",
    "            batch_captions = captions[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_captions, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            cls_tokens = last_hidden_state[:, 0, :]  # Extract CLS token\n",
    "            features.append(cls_tokens.cpu())\n",
    "\n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "# -------------------- Training the ViT Model on CIFAR-10 --------------------\n",
    "\n",
    "def train_vit_on_cifar():\n",
    "    \"\"\"Train ViT on CIFAR-10 and save checkpoints.\"\"\"\n",
    "    model = timm.create_model(VIT_MODEL_NAME, pretrained=True, num_classes=10)\n",
    "    processor = AutoImageProcessor.from_pretrained(VIT_MODEL_NAME)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=\"./cifar_data\", train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Save checkpoint every SAVE_EVERY epochs\n",
    "        if (epoch + 1) % SAVE_EVERY == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "            checkpoint_path = os.path.join(VIT_CHECKPOINT_DIR, f\"vit_epoch_{epoch + 1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            logger.info(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        logger.info(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "# -------------------- Compute Alignment Scores --------------------\n",
    "\n",
    "def compute_alignment_between_vit_llm(vit_features, llm_features):\n",
    "    \"\"\"Compute alignment metrics between ViT and LLM features.\"\"\"\n",
    "    alignment_score = AlignmentMetrics.mutual_knn(vit_features, llm_features, topk=50)\n",
    "    return alignment_score\n",
    "\n",
    "def process_alignment_scores(vit_checkpoints, llm_model_checkpoints, images, captions):\n",
    "    \"\"\"Extract features from each model and compute alignment scores.\"\"\"\n",
    "    alignment_records = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Extract ViT features from saved checkpoints\n",
    "    processor = AutoImageProcessor.from_pretrained(VIT_MODEL_NAME)\n",
    "    for vit_epoch in vit_checkpoints:\n",
    "        checkpoint_path = os.path.join(VIT_CHECKPOINT_DIR, f\"vit_epoch_{vit_epoch}.pth\")\n",
    "        model = timm.create_model(VIT_MODEL_NAME, pretrained=False, num_classes=10)\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "        vit_features = extract_vit_features(model, processor, images)\n",
    "        vit_save_path = os.path.join(VIT_FEATURES_DIR, f\"vit_epoch_{vit_epoch}.pt\")\n",
    "        torch.save(vit_features, vit_save_path)\n",
    "\n",
    "        for llm_name, checkpoints in llm_model_checkpoints.items():\n",
    "            for checkpoint in checkpoints:\n",
    "                # Load LLM and extract features\n",
    "                llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    f\"EleutherAI/{llm_name}\", revision=checkpoint\n",
    "                ).to(device)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/{llm_name}\")\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                llm_features = extract_llm_features(llm_model, tokenizer, captions)\n",
    "                \n",
    "                # Compute alignment\n",
    "                alignment_score = compute_alignment_between_vit_llm(vit_features, llm_features)\n",
    "                alignment_records.append({\n",
    "                    'VIT Epoch': vit_epoch,\n",
    "                    'LLM Name': llm_name,\n",
    "                    'LLM Checkpoint': checkpoint,\n",
    "                    'Alignment Score': alignment_score\n",
    "                })\n",
    "                logger.info(f\"Alignment score between ViT (epoch {vit_epoch}) and {llm_name} ({checkpoint}): {alignment_score}\")\n",
    "\n",
    "    return pd.DataFrame(alignment_records)\n",
    "\n",
    "# -------------------- Plotting the Results --------------------\n",
    "\n",
    "def plot_alignment_scores(alignment_df):\n",
    "    \"\"\"Plot alignment scores over training steps.\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.lineplot(\n",
    "        data=alignment_df,\n",
    "        x='LLM Checkpoint',\n",
    "        y='Alignment Score',\n",
    "        hue='VIT Epoch',\n",
    "        marker='o'\n",
    "    )\n",
    "    plt.title('Alignment Scores between ViT and LLM Checkpoints')\n",
    "    plt.xlabel('LLM Training Step')\n",
    "    plt.ylabel('Alignment Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend(title='ViT Epoch')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------- Main Execution --------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_vit_on_cifar()\n",
    "    vit_checkpoints = list(range(5, NUM_EPOCHS + 1, SAVE_EVERY))  # Epoch checkpoints for ViT\n",
    "    alignment_df = process_alignment_scores(vit_checkpoints, LLM_MODEL_CHECKPOINTS, images, captions)\n",
    "    alignment_df.to_csv('alignment_scores_vit_llm.csv', index=False)\n",
    "    plot_alignment_scores(alignment_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da989e41-f704-4be4-b0a5-9ed78794b90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
