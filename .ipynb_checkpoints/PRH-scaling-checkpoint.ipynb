{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f5aa3e-4aad-44ce-993a-8a7ddba3eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step0', 'step1000', 'step8000', 'step15000', 'step22000', 'step29000', 'step36000', 'step43000', 'step50000', 'step57000', 'step64000', 'step71000', 'step78000', 'step85000', 'step92000', 'step99000', 'step106000', 'step113000', 'step120000', 'step127000', 'step134000', 'step141000', 'step143000']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from metrics import AlignmentMetrics\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "\n",
    "# Define model checkpoints with these adjusted steps\n",
    "all_checkpoints = sorted(\n",
    "    {f\"step{step}\" for step in [0, 1000] + list(range(8000, 15000, 7000)) + list(range(15000, 143000, 7000)) + [143000]},\n",
    "    key=lambda x: int(x.replace(\"step\", \"\"))\n",
    ")\n",
    "print(all_checkpoints)\n",
    "\n",
    "model_checkpoints = {\n",
    "    \"pythia-1.4b\": all_checkpoints,\n",
    "    \"pythia-1b\": all_checkpoints,\n",
    "    \"pythia-410m\": all_checkpoints,\n",
    "    \"pythia-160m\": all_checkpoints,\n",
    "    \"pythia-70m\": all_checkpoints,\n",
    "}\n",
    "\n",
    "# Directory to store extracted features\n",
    "FEATURES_DIR = \"model_features\"\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# Load a sentence classification dataset (e.g., SST-2 from GLUE)\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:1000]\")  # Using 1000 samples for faster processing\n",
    "sentences = dataset[\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35d43f-3f6f-4c84-9bdc-80a915213138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoints and extracting features:   0%|                                                                    | 0/5 [00:00<?, ?it/s]\n",
      "Feature extraction for pythia-1.4b:   0%|                                                                            | 0/23 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b4585bb7b64467bb4e6716ccf210a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08777edb11114cccac06adf88b0af6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d7f935c1ce462a9293997ddf9e48fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4779bb6016554ebe9924a093a4c4c453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66509a5cec9f4c6fbe9074ef497972be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------- Feature Extraction Functions --------------------\n",
    "\n",
    "def extract_features(model, tokenizer, sentences, batch_size=64):\n",
    "    \"\"\"Extract last hidden state features from the model for given sentences using mixed precision.\"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    features = []\n",
    "    with torch.cuda.amp.autocast(), torch.no_grad():\n",
    "        for i in tqdm(range(0, len(sentences), batch_size), desc=\"Extracting features\", leave=False):\n",
    "            batch_sentences = sentences[i:i + batch_size]\n",
    "            inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Set the model to output hidden states\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            # Extract the last hidden state (typically the last element in hidden_states)\n",
    "            last_hidden_state = outputs.hidden_states[-1]\n",
    "            cls_tokens = last_hidden_state[:, 0, :]  # Use the first token's hidden state as a feature\n",
    "            features.append(cls_tokens.cpu())  # Move to CPU to save memory\n",
    "    \n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "# -------------------- Load Models and Extract Features --------------------\n",
    "\n",
    "def extract_features_for_checkpoint(model_name, checkpoint, sentences, batch_size=64):\n",
    "    \"\"\"Extract features for a given model at a specific checkpoint.\"\"\"\n",
    "    feature_save_path = os.path.join(FEATURES_DIR, f\"{model_name.replace('/', '_')}_{checkpoint}.pt\")\n",
    "    \n",
    "    # If features are already saved, load them\n",
    "    if os.path.exists(feature_save_path):\n",
    "        print(f\"Loading saved features for {model_name} at {checkpoint}...\")\n",
    "        return checkpoint, torch.load(feature_save_path)\n",
    "    \n",
    "    # Load model and corresponding tokenizer with the specific checkpoint revision\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"EleutherAI/{model_name}\", revision=checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/{model_name}\", revision=checkpoint)\n",
    "    \n",
    "    # Ensure the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(model, tokenizer, sentences, batch_size)\n",
    "    \n",
    "    # Save the extracted features to disk\n",
    "    torch.save(features, feature_save_path)\n",
    "    print(f\"Saved features for {model_name} at {checkpoint} to {feature_save_path}\")\n",
    "    \n",
    "    # Free model and tokenizer from GPU memory\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return checkpoint, features\n",
    "\n",
    "# -------------------- Process Models Across Checkpoints --------------------\n",
    "\n",
    "model_features = {}\n",
    "batch_size = 128  # Adjust based on available GPU memory\n",
    "max_workers = 4  # Number of models to process concurrently\n",
    "\n",
    "# Load all checkpoints and extract features\n",
    "for model_name, checkpoints in tqdm(model_checkpoints.items(), desc=\"Loading checkpoints and extracting features\"):\n",
    "    model_features[model_name] = {}\n",
    "    \n",
    "    # Process each model's checkpoints in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as model_executor:\n",
    "        futures = {\n",
    "            model_executor.submit(extract_features_for_checkpoint, model_name, checkpoint, sentences, batch_size): (model_name, checkpoint)\n",
    "            for checkpoint in checkpoints\n",
    "        }\n",
    "        \n",
    "        # Gather results as they complete\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Feature extraction for {model_name}\"):\n",
    "            model_name, checkpoint = futures[future]\n",
    "            try:\n",
    "                checkpoint, features = future.result()\n",
    "                model_features[model_name][checkpoint] = features\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Processing {model_name} at {checkpoint}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1eb8d4-6b3f-470d-8fd6-ac6c449e9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Alignment Metric Computation --------------------\n",
    "\n",
    "alignment_records = []\n",
    "\n",
    "# Calculate alignment scores at different checkpoints, ensuring checkpoints exist for both models\n",
    "common_checkpoints = set(all_checkpoints)\n",
    "for model_name in model_features.keys():\n",
    "    common_checkpoints.intersection_update(model_features[model_name].keys())\n",
    "\n",
    "# Process alignment across all model pairs at each common checkpoint\n",
    "for checkpoint in tqdm(sorted(common_checkpoints), desc=\"Calculating alignment scores\"):\n",
    "    for model_a, model_b in itertools.combinations(model_features.keys(), 2):\n",
    "        try:\n",
    "            features_a = model_features[model_a][checkpoint]\n",
    "            features_b = model_features[model_b][checkpoint]\n",
    "            score = AlignmentMetrics.mutual_knn(features_a, features_b, topk=50)\n",
    "            \n",
    "            alignment_records.append({\n",
    "                'Model A': model_a,\n",
    "                'Model B': model_b,\n",
    "                'Checkpoint': checkpoint,\n",
    "                'Score': score\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Calculating alignment score for {model_a} and {model_b} at {checkpoint}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ca28c-6aec-41bc-a1b2-2f4bfb6d15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from results\n",
    "score_df = pd.DataFrame(alignment_records)\n",
    "\n",
    "# -------------------- Plotting the Results --------------------\n",
    "\n",
    "# Plot alignment scores as training progresses\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=score_df, x='Checkpoint', y='Score', hue='Model A', style='Model B', markers=True)\n",
    "plt.title('Alignment Score (mutual_knn) vs. Training Checkpoint')\n",
    "plt.xlabel('Training Checkpoint')\n",
    "plt.ylabel('Alignment Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
