{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dcf823c-f7d4-49f5-94d2-52b480957d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a folder full of images whre the image-classifiers are wrong\n",
    "# prepare VIT models\n",
    "# compute alignment scores\n",
    "# get spearman correlation \n",
    "# plot the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c03e2b-3df9-4b1b-97d7-f4e35146594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    \"vit_tiny_patch16_224.augreg_in21k\",\n",
    "    \"vit_small_patch16_224.augreg_in21k\",\n",
    "    \"vit_base_patch16_224.augreg_in21k\",\n",
    "    \"vit_large_patch16_224.augreg_in21k\",\n",
    "    \"vit_base_patch16_224.mae\",\n",
    "    \"vit_large_patch16_224.mae\",\n",
    "    \"vit_huge_patch14_224.mae\",\n",
    "    \"vit_small_patch14_dinov2.lvd142m\",\n",
    "    \"vit_base_patch14_dinov2.lvd142m\",\n",
    "    \"vit_large_patch14_dinov2.lvd142m\",\n",
    "    \"vit_giant_patch14_dinov2.lvd142m\",\n",
    "    \"vit_base_patch16_clip_224.laion2b\",\n",
    "    \"vit_large_patch14_clip_224.laion2b\",\n",
    "    \"vit_huge_patch14_clip_224.laion2b\",\n",
    "    \"vit_base_patch16_clip_224.laion2b_ft_in12k\",\n",
    "    \"vit_large_patch14_clip_224.laion2b_ft_in12k\",\n",
    "    \"vit_huge_patch14_clip_224.laion2b_ft_in12k\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73cff9bb-547b-4bc7-8ced-aa6639c736f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from metrics import AlignmentMetrics\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# -------------------- Initialization --------------------\n",
    "\n",
    "EXPERIMENT_DIR = \"places365/experiment_samples\"\n",
    "NOISE_STEPS_DIR = \"noisy_images_steps\"\n",
    "os.makedirs(NOISE_STEPS_DIR, exist_ok=True)\n",
    "\n",
    "FEATURES_DIR = \"model_features\"\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# Load images from the experiment directory\n",
    "image_files = os.listdir(EXPERIMENT_DIR)\n",
    "image_files = sorted(image_files)  # Use all selected images, no shuffling or limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e326a160-b930-4d17-b318-03199ef59e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Image Loading --------------------\n",
    "\n",
    "def load_image_as_array(image_path):\n",
    "    \"\"\"Load an image, convert to RGB, and return as a NumPy array normalized to [0, 1].\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "            img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
    "            return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all images as NumPy arrays\n",
    "loaded_images = [load_image_as_array(os.path.join(EXPERIMENT_DIR, img_file)) for img_file in image_files]\n",
    "loaded_images = [img for img in loaded_images if img is not None]\n",
    "\n",
    "# --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488ad005-184b-4905-8c0c-b07e49ebfc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Noise Addition Function --------------------\n",
    "\n",
    "# def save_noisy_images_step(images, step, total_steps):\n",
    "#     \"\"\"Blend images with random noise and save them in a folder named after the noise step.\"\"\"\n",
    "#     alpha = step / total_steps  # Blending factor, increases linearly\n",
    "#     step_dir = os.path.join(NOISE_STEPS_DIR, f\"step_{step:03d}\")\n",
    "#     os.makedirs(step_dir, exist_ok=True)\n",
    "    \n",
    "#     for idx, img in enumerate(images):\n",
    "#         random_noise = np.random.rand(*img.shape)  # Random noise image\n",
    "#         noisy_image = (1 - alpha) * img + alpha * random_noise\n",
    "#         noisy_image = np.clip(noisy_image, 0, 1)  # Ensure values remain in [0, 1]\n",
    "#         noisy_image_pil = Image.fromarray((noisy_image * 255).astype('uint8'))\n",
    "#         noisy_image_pil.save(os.path.join(step_dir, f\"image_{idx:04d}.png\"))\n",
    "\n",
    "# # Parallel noise addition and saving\n",
    "# total_steps = 100\n",
    "# with ThreadPoolExecutor() as executor:\n",
    "#     futures = [executor.submit(save_noisy_images_step, loaded_images, step, total_steps) for step in range(total_steps)]\n",
    "#     for future in tqdm(as_completed(futures), total=total_steps, desc=\"Adding noise to images\"):\n",
    "#         future.result()  # Will raise any exceptions caught during the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b649b0-8b77-4be1-8229-8702694adb9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|                                                                                             | 0/17 [00:00<?, ?it/s]2024-10-23 23:40:52,449 [INFO] Processing model: vit_tiny_patch16_224.augreg_in21k\n",
      "2024-10-23 23:40:52,570 [INFO] Loading pretrained weights from Hugging Face hub (timm/vit_tiny_patch16_224.augreg_in21k)\n",
      "2024-10-23 23:40:52,810 [INFO] [timm/vit_tiny_patch16_224.augreg_in21k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2024-10-23 23:40:52,824 [INFO] Model vit_tiny_patch16_224.augreg_in21k initialized successfully.\n",
      "/tmp/ipykernel_30745/3406457896.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from metrics import AlignmentMetrics\n",
    "import itertools\n",
    "import gc  # Garbage collector for memory management\n",
    "import logging\n",
    "\n",
    "# -------------------- Setup Logging --------------------\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"feature_extraction.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -------------------- Helper Functions --------------------\n",
    "\n",
    "def get_dynamic_settings(model_name, gpu_memory_gb=24):\n",
    "    \"\"\"\n",
    "    Adjust batch size based on model size to fit within GPU memory constraints.\n",
    "    \"\"\"\n",
    "    large_models = [\"vit_large\", \"vit_huge\", \"vit_giant\", \"dinov2\", \"clip\"]\n",
    "    medium_models = [\"vit_base\"]\n",
    "    small_models = [\"vit_small\", \"vit_tiny\"]\n",
    "    \n",
    "    # Default batch size\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Adjust batch size based on model size\n",
    "    if any(keyword in model_name for keyword in large_models):\n",
    "        batch_size = 8  # Further reduced for very large models\n",
    "    elif any(keyword in model_name for keyword in medium_models):\n",
    "        batch_size = 16\n",
    "    elif any(keyword in model_name for keyword in small_models):\n",
    "        batch_size = 32\n",
    "    else:\n",
    "        batch_size = 16  # Fallback batch size\n",
    "    \n",
    "    return batch_size\n",
    "\n",
    "# -------------------- Feature Extraction Functions --------------------\n",
    "\n",
    "def extract_features(model, images, batch_size=32):\n",
    "    \"\"\"Extract CLS token features from a model for given images using mixed precision.\"\"\"\n",
    "    features = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            for i in (range(0, len(images), batch_size)):\n",
    "                batch = torch.stack(images[i:i + batch_size]).to(device)\n",
    "                outputs = model.forward_features(batch)\n",
    "                cls_tokens = F.normalize(outputs[:, 0, :], dim=-1)\n",
    "                features.append(cls_tokens.cpu())  # Move to CPU to save GPU memory\n",
    "    \n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "# -------------------- Load Noisy Images and Extract Features --------------------\n",
    "\n",
    "def load_images_from_step(step_dir, target_size=(224, 224)):\n",
    "    \"\"\"Load all images from a given noise step directory and resize them to the target size.\"\"\"\n",
    "    image_files = sorted([f for f in os.listdir(step_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    images = []\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(step_dir, img_file)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert(\"RGB\").resize(target_size)  # Resize to target size\n",
    "                img_tensor = torch.tensor(np.array(img) / 255.0, dtype=torch.float32).permute(2, 0, 1)  # Convert to tensor\n",
    "                images.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error loading image {img_path}: {e}\")\n",
    "    return images\n",
    "\n",
    "def extract_features_for_step(model, step_dir, step, batch_size=32):\n",
    "    \"\"\"Extract features for a given model and noise step.\"\"\"\n",
    "    images = load_images_from_step(step_dir)\n",
    "    if not images:\n",
    "        raise ValueError(f\"No images found in {step_dir}\")\n",
    "    \n",
    "    features = extract_features(model, images, batch_size)\n",
    "    del images  # Free up memory used by images\n",
    "    torch.cuda.empty_cache()  # Clear any remaining GPU cache\n",
    "    gc.collect()  # Trigger garbage collection\n",
    "    return step, features\n",
    "\n",
    "# -------------------- Process Model Function --------------------\n",
    "\n",
    "def process_model(model_name, steps_to_process, NOISE_STEPS_DIR, gpu_memory_gb=24):\n",
    "    \"\"\"Process feature extraction for a model across specified steps.\"\"\"\n",
    "    logging.info(f\"Processing model: {model_name}\")\n",
    "    batch_size = get_dynamic_settings(model_name, gpu_memory_gb)\n",
    "    noise_features = {}\n",
    "    feature_save_path = f\"{model_name}_features.pt\"\n",
    "    \n",
    "    # Check if features for this model are already saved\n",
    "    if os.path.exists(feature_save_path):\n",
    "        logging.info(f\"Loading saved features for {model_name} from {feature_save_path}...\")\n",
    "        noise_features = torch.load(feature_save_path)\n",
    "        return noise_features\n",
    "    \n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "        logging.info(f\"Model {model_name} initialized successfully.\")\n",
    "        \n",
    "        for step in steps_to_process:\n",
    "            step_dir = os.path.join(NOISE_STEPS_DIR, f\"step_{step:03d}\")\n",
    "            if not os.path.isdir(step_dir):\n",
    "                logging.warning(f\"Directory {step_dir} does not exist. Skipping step {step}.\")\n",
    "                continue\n",
    "            try:\n",
    "                step_num, features = extract_features_for_step(model, step_dir, step, batch_size)\n",
    "                noise_features[step_num] = features\n",
    "                torch.save(noise_features, feature_save_path)  # Save incrementally\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing step {step} for model {model_name}: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error initializing model {model_name}: {e}\")\n",
    "    finally:\n",
    "        # Ensure model is deleted and memory is freed\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            logging.info(f\"Cleared GPU memory after processing model {model_name}.\")\n",
    "    \n",
    "    return noise_features\n",
    "\n",
    "# -------------------- Alignment Metric Computation --------------------\n",
    "\n",
    "def compute_alignment_scores(model_features_noise, model_list, steps_to_process):\n",
    "    alignment_records = []\n",
    "    \n",
    "    # Calculate alignment scores at different noise levels\n",
    "    for step in steps_to_process:\n",
    "        for model_a, model_b in itertools.combinations(model_list, 2):\n",
    "            try:\n",
    "                features_a = model_features_noise[model_a][step]\n",
    "                features_b = model_features_noise[model_b][step]\n",
    "                score = AlignmentMetrics.mutual_knn(features_a, features_b, topk=50)\n",
    "                \n",
    "                alignment_records.append({\n",
    "                    'Model A': model_a,\n",
    "                    'Model B': model_b,\n",
    "                    'Step': step,\n",
    "                    'Score': score\n",
    "                })\n",
    "            except KeyError as ke:\n",
    "                logging.warning(f\"Missing features for step {step}: {ke}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error calculating alignment score for models {model_a} and {model_b} at step {step}: {e}\")\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    score_df = pd.DataFrame(alignment_records)\n",
    "    return score_df\n",
    "\n",
    "# -------------------- Plotting the Results --------------------\n",
    "\n",
    "def plot_results(score_df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(data=score_df, x='Step', y='Score', hue='Model A', style='Model B', markers=True)\n",
    "    plt.title('Alignment Score (mutual_knn) vs. Noise Level')\n",
    "    plt.xlabel('Noise Step (0 = original, 100 = random noise)')\n",
    "    plt.ylabel('Alignment Score')\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------- Main Execution --------------------\n",
    "\n",
    "def main():\n",
    "    model_list = [\n",
    "        \"vit_tiny_patch16_224.augreg_in21k\",\n",
    "        \"vit_small_patch16_224.augreg_in21k\",\n",
    "        \"vit_base_patch16_224.augreg_in21k\",\n",
    "        \"vit_large_patch16_224.augreg_in21k\",\n",
    "        \"vit_base_patch16_224.mae\",\n",
    "        \"vit_large_patch16_224.mae\",\n",
    "        \"vit_huge_patch14_224.mae\",\n",
    "        \"vit_small_patch14_dinov2.lvd142m\",\n",
    "        \"vit_base_patch14_dinov2.lvd142m\",\n",
    "        \"vit_large_patch14_dinov2.lvd142m\",\n",
    "        \"vit_giant_patch14_dinov2.lvd142m\",\n",
    "        \"vit_base_patch16_clip_224.laion2b\",\n",
    "        \"vit_large_patch14_clip_224.laion2b\",\n",
    "        \"vit_huge_patch14_clip_224.laion2b\",\n",
    "        \"vit_base_patch16_clip_224.laion2b_ft_in12k\",\n",
    "        \"vit_large_patch14_clip_224.laion2b_ft_in12k\",\n",
    "        \"vit_huge_patch14_clip_224.laion2b_ft_in12k\",\n",
    "    ]\n",
    "    \n",
    "    gpu_memory_gb = 24  # GPU memory in GB\n",
    "    steps_to_process = range(0, 100, 5)  # Process steps with an interval of 5 (0, 5, 10, ..., 95)\n",
    "    \n",
    "    model_features_noise = {}\n",
    "    \n",
    "    for model_name in tqdm(model_list, desc=\"Processing models\"):\n",
    "        features = process_model(model_name, steps_to_process, NOISE_STEPS_DIR, gpu_memory_gb)\n",
    "        model_features_noise[model_name] = features\n",
    "    \n",
    "    # -------------------- Alignment Metric Computation --------------------\n",
    "    \n",
    "    score_df = compute_alignment_scores(model_features_noise, model_list, steps_to_process)\n",
    "    \n",
    "    # -------------------- Plotting the Results --------------------\n",
    "    \n",
    "    plot_results(score_df)\n",
    "    logging.info(\"Feature extraction and alignment computation completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ac2c2-debf-41f8-a5c4-edd77f44c721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
