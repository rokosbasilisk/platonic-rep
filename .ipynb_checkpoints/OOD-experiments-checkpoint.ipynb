{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dcf823c-f7d4-49f5-94d2-52b480957d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a folder full of images whre the image-classifiers are wrong\n",
    "# prepare VIT models\n",
    "# compute alignment scores\n",
    "# get spearman correlation \n",
    "# plot the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f04964-6322-4c67-bb93-51774ab107f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09933c22b670419b9c93a5c22f2c314c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all images from the test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 2000/2000 [00:06<00:00, 314.33it/s]\n",
      "Extracting features for all models:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_598374/2438627170.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  features = torch.load(feature_path, map_location='cpu')\n",
      "Extracting features for all models: 100%|██████████| 17/17 [00:00<00:00, 85.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features for vit_tiny_patch16_224.augreg_in21k from model_features/vit_tiny_patch16_224.augreg_in21k_features.pt...\n",
      "Features for vit_tiny_patch16_224.augreg_in21k loaded successfully, shape: torch.Size([2000, 192])\n",
      "Loading features for vit_small_patch16_224.augreg_in21k from model_features/vit_small_patch16_224.augreg_in21k_features.pt...\n",
      "Features for vit_small_patch16_224.augreg_in21k loaded successfully, shape: torch.Size([2000, 384])\n",
      "Loading features for vit_base_patch16_224.augreg_in21k from model_features/vit_base_patch16_224.augreg_in21k_features.pt...\n",
      "Features for vit_base_patch16_224.augreg_in21k loaded successfully, shape: torch.Size([2000, 768])\n",
      "Loading features for vit_large_patch16_224.augreg_in21k from model_features/vit_large_patch16_224.augreg_in21k_features.pt...\n",
      "Features for vit_large_patch16_224.augreg_in21k loaded successfully, shape: torch.Size([2000, 1024])\n",
      "Loading features for vit_base_patch16_224.mae from model_features/vit_base_patch16_224.mae_features.pt...\n",
      "Features for vit_base_patch16_224.mae loaded successfully, shape: torch.Size([2000, 768])\n",
      "Loading features for vit_large_patch16_224.mae from model_features/vit_large_patch16_224.mae_features.pt...\n",
      "Features for vit_large_patch16_224.mae loaded successfully, shape: torch.Size([2000, 1024])\n",
      "Loading features for vit_huge_patch14_224.mae from model_features/vit_huge_patch14_224.mae_features.pt...\n",
      "Features for vit_huge_patch14_224.mae loaded successfully, shape: torch.Size([2000, 1280])\n",
      "Loading features for vit_small_patch14_dinov2.lvd142m from model_features/vit_small_patch14_dinov2.lvd142m_features.pt...\n",
      "Features for vit_small_patch14_dinov2.lvd142m loaded successfully, shape: torch.Size([2000, 384])\n",
      "Loading features for vit_base_patch14_dinov2.lvd142m from model_features/vit_base_patch14_dinov2.lvd142m_features.pt...\n",
      "Features for vit_base_patch14_dinov2.lvd142m loaded successfully, shape: torch.Size([2000, 768])\n",
      "Loading features for vit_large_patch14_dinov2.lvd142m from model_features/vit_large_patch14_dinov2.lvd142m_features.pt...\n",
      "Features for vit_large_patch14_dinov2.lvd142m loaded successfully, shape: torch.Size([2000, 1024])\n",
      "Loading features for vit_giant_patch14_dinov2.lvd142m from model_features/vit_giant_patch14_dinov2.lvd142m_features.pt...\n",
      "Features for vit_giant_patch14_dinov2.lvd142m loaded successfully, shape: torch.Size([2000, 1536])\n",
      "Loading features for vit_base_patch16_clip_224.laion2b from model_features/vit_base_patch16_clip_224.laion2b_features.pt...\n",
      "Features for vit_base_patch16_clip_224.laion2b loaded successfully, shape: torch.Size([2000, 768])\n",
      "Loading features for vit_large_patch14_clip_224.laion2b from model_features/vit_large_patch14_clip_224.laion2b_features.pt...\n",
      "Features for vit_large_patch14_clip_224.laion2b loaded successfully, shape: torch.Size([2000, 1024])\n",
      "Loading features for vit_huge_patch14_clip_224.laion2b from model_features/vit_huge_patch14_clip_224.laion2b_features.pt...\n",
      "Features for vit_huge_patch14_clip_224.laion2b loaded successfully, shape: torch.Size([2000, 1280])\n",
      "Loading features for vit_base_patch16_clip_224.laion2b_ft_in12k from model_features/vit_base_patch16_clip_224.laion2b_ft_in12k_features.pt...\n",
      "Features for vit_base_patch16_clip_224.laion2b_ft_in12k loaded successfully, shape: torch.Size([2000, 768])\n",
      "Loading features for vit_large_patch14_clip_224.laion2b_ft_in12k from model_features/vit_large_patch14_clip_224.laion2b_ft_in12k_features.pt...\n",
      "Features for vit_large_patch14_clip_224.laion2b_ft_in12k loaded successfully, shape: torch.Size([2000, 1024])\n",
      "Loading features for vit_huge_patch14_clip_224.laion2b_ft_in12k from model_features/vit_huge_patch14_clip_224.laion2b_ft_in12k_features.pt...\n",
      "Features for vit_huge_patch14_clip_224.laion2b_ft_in12k loaded successfully, shape: torch.Size([2000, 1280])\n",
      "\n",
      "Verification of extracted features:\n",
      "Model vit_tiny_patch16_224.augreg_in21k: 2000 samples, 192 features.\n",
      "Model vit_small_patch16_224.augreg_in21k: 2000 samples, 384 features.\n",
      "Model vit_base_patch16_224.augreg_in21k: 2000 samples, 768 features.\n",
      "Model vit_large_patch16_224.augreg_in21k: 2000 samples, 1024 features.\n",
      "Model vit_base_patch16_224.mae: 2000 samples, 768 features.\n",
      "Model vit_large_patch16_224.mae: 2000 samples, 1024 features.\n",
      "Model vit_huge_patch14_224.mae: 2000 samples, 1280 features.\n",
      "Model vit_small_patch14_dinov2.lvd142m: 2000 samples, 384 features.\n",
      "Model vit_base_patch14_dinov2.lvd142m: 2000 samples, 768 features.\n",
      "Model vit_large_patch14_dinov2.lvd142m: 2000 samples, 1024 features.\n",
      "Model vit_giant_patch14_dinov2.lvd142m: 2000 samples, 1536 features.\n",
      "Model vit_base_patch16_clip_224.laion2b: 2000 samples, 768 features.\n",
      "Model vit_large_patch14_clip_224.laion2b: 2000 samples, 1024 features.\n",
      "Model vit_huge_patch14_clip_224.laion2b: 2000 samples, 1280 features.\n",
      "Model vit_base_patch16_clip_224.laion2b_ft_in12k: 2000 samples, 768 features.\n",
      "Model vit_large_patch14_clip_224.laion2b_ft_in12k: 2000 samples, 1024 features.\n",
      "Model vit_huge_patch14_clip_224.laion2b_ft_in12k: 2000 samples, 1280 features.\n",
      "\n",
      "Models with sufficient samples (17):\n",
      " - vit_tiny_patch16_224.augreg_in21k: 2000 samples\n",
      " - vit_small_patch16_224.augreg_in21k: 2000 samples\n",
      " - vit_base_patch16_224.augreg_in21k: 2000 samples\n",
      " - vit_large_patch16_224.augreg_in21k: 2000 samples\n",
      " - vit_base_patch16_224.mae: 2000 samples\n",
      " - vit_large_patch16_224.mae: 2000 samples\n",
      " - vit_huge_patch14_224.mae: 2000 samples\n",
      " - vit_small_patch14_dinov2.lvd142m: 2000 samples\n",
      " - vit_base_patch14_dinov2.lvd142m: 2000 samples\n",
      " - vit_large_patch14_dinov2.lvd142m: 2000 samples\n",
      " - vit_giant_patch14_dinov2.lvd142m: 2000 samples\n",
      " - vit_base_patch16_clip_224.laion2b: 2000 samples\n",
      " - vit_large_patch14_clip_224.laion2b: 2000 samples\n",
      " - vit_huge_patch14_clip_224.laion2b: 2000 samples\n",
      " - vit_base_patch16_clip_224.laion2b_ft_in12k: 2000 samples\n",
      " - vit_large_patch14_clip_224.laion2b_ft_in12k: 2000 samples\n",
      " - vit_huge_patch14_clip_224.laion2b_ft_in12k: 2000 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import timm\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------- Initialization --------------------\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"cais/imagenet-o\")\n",
    "\n",
    "# Load all images from the test split\n",
    "print(\"Loading all images from the test split...\")\n",
    "sampled_images = [ds['test'][idx]['image'] for idx in tqdm(range(len(ds['test'])), desc=\"Loading images\")]\n",
    "\n",
    "# List of models to pre-download weights for\n",
    "model_list = [\n",
    "    \"vit_tiny_patch16_224.augreg_in21k\",\n",
    "    \"vit_small_patch16_224.augreg_in21k\",\n",
    "    \"vit_base_patch16_224.augreg_in21k\",\n",
    "    \"vit_large_patch16_224.augreg_in21k\",\n",
    "    \"vit_base_patch16_224.mae\",\n",
    "    \"vit_large_patch16_224.mae\",\n",
    "    \"vit_huge_patch14_224.mae\",\n",
    "    \"vit_small_patch14_dinov2.lvd142m\",\n",
    "    \"vit_base_patch14_dinov2.lvd142m\",\n",
    "    \"vit_large_patch14_dinov2.lvd142m\",\n",
    "    \"vit_giant_patch14_dinov2.lvd142m\",\n",
    "    \"vit_base_patch16_clip_224.laion2b\",\n",
    "    \"vit_large_patch14_clip_224.laion2b\",\n",
    "    \"vit_huge_patch14_clip_224.laion2b\",\n",
    "    \"vit_base_patch16_clip_224.laion2b_ft_in12k\",\n",
    "    \"vit_large_patch14_clip_224.laion2b_ft_in12k\",\n",
    "    \"vit_huge_patch14_clip_224.laion2b_ft_in12k\",\n",
    "]\n",
    "\n",
    "# Directory to store saved features\n",
    "FEATURES_DIR = \"model_features\"\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- Feature Extraction Functions --------------------\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    \"\"\"\n",
    "    Convert a PIL image to RGB if it's not already.\n",
    "    \"\"\"\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def extract_features(model_name, images, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extract the CLS token from the last layer of the model for all images.\n",
    "    \"\"\"\n",
    "    feature_path = os.path.join(FEATURES_DIR, f\"{model_name}_features.pt\")\n",
    "    \n",
    "    # Check if features are already extracted\n",
    "    if os.path.exists(feature_path):\n",
    "        print(f\"Loading existing features for {model_name}...\")\n",
    "        features = torch.load(feature_path)\n",
    "        return features\n",
    "    \n",
    "    print(f\"Extracting features for {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define preprocessing transforms based on model's default config\n",
    "    input_size = model.default_cfg.get('input_size', (3, 224, 224))\n",
    "    transform = timm.data.transforms_factory.create_transform(\n",
    "        input_size=input_size,\n",
    "        is_training=False\n",
    "    )\n",
    "    \n",
    "    features = []\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch_images = images[i:i + batch_size]\n",
    "            try:\n",
    "                # Apply transforms and stack into a batch tensor\n",
    "                batch = torch.stack([transform(convert_to_rgb(img)) for img in batch_images]).to(device)\n",
    "                \n",
    "                # Forward pass to get features\n",
    "                outputs = model.forward_features(batch)  # Shape: (batch_size, num_tokens, feature_dim)\n",
    "                \n",
    "                # Extract CLS token (first token)\n",
    "                cls_tokens = outputs[:, 0, :]  # Shape: (batch_size, feature_dim)\n",
    "                \n",
    "                # Normalize features\n",
    "                cls_tokens = F.normalize(cls_tokens, dim=-1)\n",
    "                \n",
    "                features.append(cls_tokens.cpu())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {i // batch_size} for {model_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if features:\n",
    "        # Concatenate all features\n",
    "        features = torch.cat(features, dim=0)  # Shape: (num_images, feature_dim)\n",
    "        \n",
    "        # Save features to disk\n",
    "        torch.save(features, feature_path)\n",
    "        print(f\"Features for {model_name} saved to {feature_path}\")\n",
    "        return features\n",
    "    else:\n",
    "        print(f\"No features extracted for {model_name}.\")\n",
    "        return None\n",
    "\n",
    "def load_or_extract_features(model_name, images, batch_size=32):\n",
    "    \"\"\"\n",
    "    Loads the precomputed features for a model if they exist.\n",
    "    Otherwise, extracts the features and saves them.\n",
    "    \"\"\"\n",
    "    feature_path = os.path.join(FEATURES_DIR, f\"{model_name}_features.pt\")\n",
    "    if os.path.exists(feature_path):\n",
    "        print(f\"Loading features for {model_name} from {feature_path}...\")\n",
    "        features = torch.load(feature_path, map_location='cpu')\n",
    "        print(f\"Features for {model_name} loaded successfully, shape: {features.shape}\")\n",
    "        return features\n",
    "    return extract_features(model_name, images, batch_size)\n",
    "\n",
    "# -------------------- Feature Extraction Phase --------------------\n",
    "\n",
    "# Extract features for all models\n",
    "model_features = {}\n",
    "\n",
    "for model_name in tqdm(model_list, desc=\"Extracting features for all models\"):\n",
    "    features = load_or_extract_features(model_name, sampled_images, batch_size=32)\n",
    "    if features is not None:\n",
    "        model_features[model_name] = features\n",
    "    else:\n",
    "        print(f\"Features for {model_name} could not be extracted.\")\n",
    "\n",
    "# -------------------- Verification Phase --------------------\n",
    "\n",
    "# Verify that each model has the expected number of samples and feature dimensions\n",
    "expected_num_samples = len(sampled_images)\n",
    "print(\"\\nVerification of extracted features:\")\n",
    "for model_name, features in model_features.items():\n",
    "    num_samples, feature_dim = features.size()\n",
    "    print(f\"Model {model_name}: {num_samples} samples, {feature_dim} features.\")\n",
    "    if num_samples != expected_num_samples:\n",
    "        print(f\"  Warning: Expected {expected_num_samples} samples, but got {num_samples}.\")\n",
    "\n",
    "# Identify models with sufficient samples (at least 10)\n",
    "sufficient_models = [model_name for model_name, features in model_features.items() \n",
    "                     if features is not None and features.size(0) >= 10]\n",
    "\n",
    "print(f\"\\nModels with sufficient samples ({len(sufficient_models)}):\")\n",
    "for model_name in sufficient_models:\n",
    "    print(f\" - {model_name}: {model_features[model_name].size(0)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ec3b3-9090-4b02-b8a9-0f6a4f177b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Alignment Score Calculation --------------------\n",
    "\n",
    "from metrics import AlignmentMetrics  # Ensure this is correctly implemented\n",
    "\n",
    "def compute_alignment_score(model_a, model_b, metric='cknna', topk=10):\n",
    "    \"\"\"\n",
    "    Compute the alignment score between two models using the specified metric.\n",
    "    \"\"\"\n",
    "    feats_a = model_features[model_a]\n",
    "    feats_b = model_features[model_b]\n",
    "    \n",
    "    if metric == 'cknna':\n",
    "        score = AlignmentMetrics.cknna(feats_a, feats_b, topk=topk)\n",
    "    elif metric == 'mutual_knn':\n",
    "        score = AlignmentMetrics.mutual_knn(feats_a, feats_b, topk=topk)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "# List of metrics to compute\n",
    "metrics_list = ['cknna', 'mutual_knn']  # Add more metrics if implemented\n",
    "\n",
    "# Generate all possible model pairs\n",
    "model_pairs = list(itertools.combinations(sufficient_models, 2))\n",
    "print(f\"\\nTotal model pairs to compute alignment scores: {len(model_pairs)}\")\n",
    "\n",
    "# Initialize a dictionary to store alignment scores\n",
    "alignment_scores = {metric: {} for metric in metrics_list}\n",
    "\n",
    "# Function to compute alignment scores for a given pair across all metrics\n",
    "def compute_scores_for_pair(pair):\n",
    "    model_a, model_b = pair\n",
    "    scores = {}\n",
    "    for metric in metrics_list:\n",
    "        try:\n",
    "            score = compute_alignment_score(model_a, model_b, metric=metric, topk=10)\n",
    "            scores[metric] = score\n",
    "            print(f\"Alignment score for ({model_a}, {model_b}) using '{metric}': {score}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing {metric} for ({model_a}, {model_b}): {e}\")\n",
    "            scores[metric] = None\n",
    "    return (model_a, model_b), scores\n",
    "\n",
    "# Use ProcessPoolExecutor for parallel alignment score computation\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "print(\"\\nCalculating alignment scores for all model pairs...\")\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Submit all alignment score computation tasks\n",
    "    futures = {executor.submit(compute_scores_for_pair, pair): pair for pair in model_pairs}\n",
    "    \n",
    "    # Use tqdm to track progress\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing alignment scores\"):\n",
    "        pair, scores = future.result()\n",
    "        for metric, score in scores.items():\n",
    "            if score is not None:\n",
    "                alignment_scores[metric][pair] = score\n",
    "\n",
    "print(\"Alignment score calculation completed.\")\n",
    "\n",
    "# -------------------- DataFrame Construction --------------------\n",
    "\n",
    "# Convert alignment_scores to a DataFrame\n",
    "records = []\n",
    "for metric, pairs_scores in alignment_scores.items():\n",
    "    for pair, score in pairs_scores.items():\n",
    "        records.append({\n",
    "            'Model A': pair[0],\n",
    "            'Model B': pair[1],\n",
    "            'Metric': metric,\n",
    "            'Score': score\n",
    "        })\n",
    "\n",
    "# Create the DataFrame\n",
    "score_df = pd.DataFrame(records)\n",
    "\n",
    "# Display a sample of the DataFrame\n",
    "print(\"\\nSample records:\")\n",
    "print(score_df.head())\n",
    "\n",
    "# -------------------- Spearman Correlation Calculation --------------------\n",
    "\n",
    "if not score_df.empty:\n",
    "    # Pivot the DataFrame to have metrics as columns\n",
    "    pivot_df = score_df.pivot(index=['Model A', 'Model B'], columns='Metric', values='Score')\n",
    "    \n",
    "    # Display a sample of the pivoted DataFrame\n",
    "    print(\"\\nSample of the pivoted DataFrame:\")\n",
    "    print(pivot_df.head())\n",
    "    \n",
    "    # Compute Spearman correlation matrix among different metrics\n",
    "    correlation_matrix = pivot_df.corr(method='spearman')\n",
    "    \n",
    "    print(\"\\nSpearman Correlation Matrix:\")\n",
    "    print(correlation_matrix)\n",
    "    \n",
    "    # -------------------- Visualization --------------------\n",
    "    \n",
    "    # Plot the Spearman correlation matrix as a heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        correlation_matrix, \n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        square=True, \n",
    "        center=0, \n",
    "        cbar_kws={'label': 'Spearman Correlation'}\n",
    "    )\n",
    "    plt.title('Spearman Correlation of Alignment Scores Across Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid alignment scores found. The DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bccedb-ad73-46ff-a661-d10b4c8f4097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
