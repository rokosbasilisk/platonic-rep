{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dcf823c-f7d4-49f5-94d2-52b480957d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a folder full of images whre the image-classifiers are wrong\n",
    "# prepare VIT models\n",
    "# compute alignment scores\n",
    "# get spearman correlation \n",
    "# plot the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6446f6cc-2cd7-4b20-85d7-f1219a0a67a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e44c5ab4fc54cce9a31240df8814590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"cais/imagenet-o\")\n",
    "\n",
    "# Randomly sample 100 image indices\n",
    "random_indices = random.sample(range(len(ds['test'])), 100)\n",
    "sampled_images = [ds['test'][idx]['image'] for idx in range(len(ds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211d2d89-a463-4d54-8e81-f11b584dfd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-downloading weights for all models in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:   6%|▌         | 1/17 [00:32<08:45, 32.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_small_patch14_dinov2.lvd142m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  12%|█▏        | 2/17 [00:35<03:42, 14.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_base_patch16_224.augreg_in21k\n",
      "Downloaded weights for vit_tiny_patch16_224.augreg_in21k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  24%|██▎       | 4/17 [00:35<01:14,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_base_patch14_dinov2.lvd142m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  29%|██▉       | 5/17 [00:36<00:50,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_base_patch16_224.mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  35%|███▌      | 6/17 [00:37<00:33,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_small_patch16_224.augreg_in21k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  41%|████      | 7/17 [00:38<00:24,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_base_patch16_clip_224.laion2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  47%|████▋     | 8/17 [00:38<00:16,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_base_patch16_clip_224.laion2b_ft_in12k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242aa25f22694da68d91b88592cfdae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  22%|##2       | 283M/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  53%|█████▎    | 9/17 [01:13<01:34, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_large_patch14_dinov2.lvd142m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  59%|█████▉    | 10/17 [01:14<00:59,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_large_patch16_224.mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  65%|██████▍   | 11/17 [01:16<00:40,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_large_patch16_224.augreg_in21k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  71%|███████   | 12/17 [01:17<00:24,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_large_patch14_clip_224.laion2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde3154eb42a473286ca7b5c54ddc7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  30%|##9       | 765M/2.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc871b664904797add32dbdb287698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:  77%|#######6  | 3.03G/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  76%|███████▋  | 13/17 [01:33<00:32,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_huge_patch14_224.mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  82%|████████▏ | 14/17 [01:43<00:26,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_giant_patch14_dinov2.lvd142m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  88%|████████▊ | 15/17 [08:17<04:08, 124.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_large_patch14_clip_224.laion2b_ft_in12k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights:  94%|█████████▍| 16/17 [08:41<01:34, 94.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_huge_patch14_clip_224.laion2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model weights: 100%|██████████| 17/17 [11:01<00:00, 38.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded weights for vit_huge_patch14_clip_224.laion2b_ft_in12k\n",
      "All model weights pre-downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of models to pre-download weights for\n",
    "model_list = [\n",
    "    \"vit_tiny_patch16_224.augreg_in21k\",\n",
    "    \"vit_small_patch16_224.augreg_in21k\",\n",
    "    \"vit_base_patch16_224.augreg_in21k\",\n",
    "    \"vit_large_patch16_224.augreg_in21k\",\n",
    "    \"vit_base_patch16_224.mae\",\n",
    "    \"vit_large_patch16_224.mae\",\n",
    "    \"vit_huge_patch14_224.mae\",\n",
    "    \"vit_small_patch14_dinov2.lvd142m\",\n",
    "    \"vit_base_patch14_dinov2.lvd142m\",\n",
    "    \"vit_large_patch14_dinov2.lvd142m\",\n",
    "    \"vit_giant_patch14_dinov2.lvd142m\",\n",
    "    \"vit_base_patch16_clip_224.laion2b\",\n",
    "    \"vit_large_patch14_clip_224.laion2b\",\n",
    "    \"vit_huge_patch14_clip_224.laion2b\",\n",
    "    \"vit_base_patch16_clip_224.laion2b_ft_in12k\",\n",
    "    \"vit_large_patch14_clip_224.laion2b_ft_in12k\",\n",
    "    \"vit_huge_patch14_clip_224.laion2b_ft_in12k\",\n",
    "]\n",
    "\n",
    "# Define a function to download weights for a single model\n",
    "def download_model_weights(model_name):\n",
    "    try:\n",
    "        # Load the model to download weights and immediately delete to free memory\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "        del model\n",
    "        return f\"Downloaded weights for {model_name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed to download weights for {model_name}: {e}\"\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel downloading\n",
    "print(\"Pre-downloading weights for all models in parallel...\")\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit all download tasks\n",
    "    futures = {executor.submit(download_model_weights, model_name): model_name for model_name in model_list}\n",
    "\n",
    "    # Use tqdm to track progress of the downloads\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading model weights\", leave=True):\n",
    "        result = future.result()\n",
    "        print(result)\n",
    "\n",
    "print(\"All model weights pre-downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539dfd1e-6121-446f-9ba1-691713b9c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Directory to store saved features\n",
    "FEATURES_DIR = \"model_features\"\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def extract_and_save_features(model_name, images, batch_size=8):\n",
    "    feature_path = os.path.join(FEATURES_DIR, f\"{model_name}_features.pt\")\n",
    "    if os.path.exists(feature_path):\n",
    "        print(f\"Features for {model_name} already exist. Loading...\")\n",
    "        return torch.load(feature_path, map_location='cpu')\n",
    "\n",
    "    print(f\"Extracting features for {model_name}...\")\n",
    "    \n",
    "    # Load the model with debug messages\n",
    "    try:\n",
    "        print(f\"Loading model {model_name}...\")\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "        print(f\"Model {model_name} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Move the model to the GPU\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Check if model is on GPU\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        print(f\"{model_name} is on GPU.\")\n",
    "    else:\n",
    "        print(f\"{model_name} is not on GPU as expected.\")\n",
    "\n",
    "    # Get the model's expected input size\n",
    "    input_size = model.default_cfg.get('input_size', (3, 224, 224))\n",
    "    img_height, img_width = input_size[1], input_size[2]\n",
    "\n",
    "    # Define a transform for preprocessing images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), desc=f\"Extracting features for {model_name}\"):\n",
    "            batch_images = images[i:i + batch_size]\n",
    "            batch_tensors = torch.stack([transform(convert_to_rgb(img)) for img in batch_images])\n",
    "            batch_tensors = batch_tensors.to(device)\n",
    "\n",
    "            try:\n",
    "                feats = model(batch_tensors).flatten(start_dim=1)\n",
    "                features.append(feats.cpu())\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference with {model_name}: {e}\")\n",
    "                return None\n",
    "\n",
    "            # Clear CUDA cache to free up memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # Concatenate and normalize features\n",
    "    features = torch.cat(features, dim=0)\n",
    "    features = torch.nn.functional.normalize(features, dim=-1)\n",
    "\n",
    "    # Save the features for future use\n",
    "    torch.save(features, feature_path)\n",
    "    print(f\"Features for {model_name} saved to {feature_path}\")\n",
    "\n",
    "    # Clear model from memory after saving\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract and save features for all models with batching\n",
    "model_features = {}\n",
    "for model_name in tqdm(model_list, desc=\"Extracting features for all models\"):\n",
    "    model_features[model_name] = extract_and_save_features(model_name, sampled_images, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1b5ce-2950-49d9-bb24-ed00a625299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import AlignmentMetrics\n",
    "import itertools\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "alignment_scores = {}\n",
    "\n",
    "# Define a function to compute alignment score for a pair of models\n",
    "def compute_alignment_score(model_pair):\n",
    "    model_a, model_b = model_pair\n",
    "    feats_a = extract_features(model_a, sampled_images)\n",
    "    feats_b = extract_features(model_b, sampled_images)\n",
    "    score = AlignmentMetrics.cknna(feats_a, feats_b, topk=10)\n",
    "    return (model_a, model_b), score\n",
    "\n",
    "# List of all model pairs\n",
    "model_pairs = list(itertools.combinations(model_list, 2))\n",
    "\n",
    "# Use ProcessPoolExecutor for parallel execution\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Submit tasks to the executor\n",
    "    futures = {executor.submit(compute_alignment_score, pair): pair for pair in model_pairs}\n",
    "    \n",
    "    # Use tqdm to track progress of the tasks as they complete\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Calculating alignment scores\", leave=True):\n",
    "        model_pair, score = future.result()\n",
    "        alignment_scores[model_pair] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020c6c6-7c99-4aa9-a9d3-16ee7f8edce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for storing scores\n",
    "score_df = pd.DataFrame(\n",
    "    list(alignment_scores.values()),\n",
    "    index=pd.MultiIndex.from_tuples(alignment_scores.keys(), names=[\"Model A\", \"Model B\"]),\n",
    "    columns=[\"Score\"]\n",
    ").unstack()\n",
    "\n",
    "# Compute Spearman correlation matrix\n",
    "correlation_matrix = score_df.corr(method='spearman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae8ba5-b520-46d1-9975-fa6fd85241cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Spearman Correlation of Alignment Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93047824-a771-4580-93ec-2f85eb6898b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Spearman Correlation of Alignment Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
